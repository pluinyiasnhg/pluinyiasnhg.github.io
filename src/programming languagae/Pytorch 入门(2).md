---
title: Pytorch 入门(2)
date: 2025-10-14
tags:
  - Pytorch
  - 尚硅谷
category:
  - Python
---
# 前言

学习尚硅谷 b站的 [NLP 教程](https://www.bilibili.com/video/BV1k44LzPEhU)。

<!-- more -->

# 技术演进历史

90 年代，随着计算能力的提升和语料资源的积累，统计方法逐渐成为主流。通过对大量文本数据进行概率建模，系统能够“学习”语言中的模式和规律。典型方法包括 n-gram 模型、隐马尔可夫模型(HMM)和最大熵模型。

进入 21 世纪，NLP 技术逐步引入传统机器学习方法，如逻辑回归、支持向量机 (SVM)、决策树、条件随机场(CRF)等。这些方法在命名实体识别、文本分类等任务上表现出色。在此阶段，特征工程成为关键环节，研究者需要设计大量手工特征来提升模型性能。

自2010年代中期开始，深度学习在NLP中迅速崛起。基于神经网络的模型RNN、LSTM、GRU等，取代了传统手工特征工程，能够从海量数据中自动提取语义表示。随后，Transformer架构的提出极大提升了语言理解与生成的能力，深度学习不仅在精度上实现突破，也推动了预训练语言模型（如GPT、BERT等）和迁移学习的发展，使NLP技术更通用、更强大。

![RNN](https://vip.123pan.cn/1844935313/obsidian/20251014224728382.png)

![LSTM（Long Short-Term Memory）](https://vip.123pan.cn/1844935313/obsidian/20251014224924676.png)

![GRU（Gated Recurrent Unit）](https://vip.123pan.cn/1844935313/obsidian/20251014224958375.png)

![Transformer|438x0](https://vip.123pan.cn/1844935313/obsidian/20251014225051167.png)

# 安装所需依赖

该课程使用的 Python 版本是 3.12。安装所需依赖有：

- `pytorch`：深度学习框架，主要用于模型的构建、训练与推理。
- `jieba`：高效的中文分词工具，用于对原始中文文本进行分词预处理。
- `gensim`：用于训练词向量模型（如 Word2Vec、FastText），提升模型对词语语义关系的理解。
- `transformers`：由 Hugging Face 提供的预训练模型库，用于加载和微调 BERT 等主流模型。
- `datasets`：Hugging Face 提供的数据处理库，用于高效加载和预处理大规模数据集。
- `TensorBoard`：可视化工具，用于展示训练过程中的损失函数、准确率等指标变化。
- `tqdm`：用于显示进度条，帮助实时监控训练与数据处理的进度。
- `Jupyter Notebook`：交互式开发环境，用于编写、测试和可视化模型代码与实验过程。

# 文本表示方法

文本表示是将自然语言转化为计算机能够理解的数值形式。

早期的文本表示方法（如词袋模型）通常将整段文本编码为一个向量。这类方法实现简单、计算高效，但存在明显的局限性——表达语序和上下文语义的能力较弱。

因此，现代 NLP 技术逐渐引入更加精细和表达力更强的文本表示方法，以更有效地建模语言的结构和含义：

 - 分词（Tokenization）是将原始文本切分为若干具有独立语义的最小单元，即 `token` 的过程，是所有 NLP 任务的起点
 - 词表（Vocabulary）是由语料库构建出的、包含模型可识别 token 的集合。词表中每个 token 都分配有唯一的 `ID`，并支持 token 与 ID 之间的双向映射
- 词向量：在训练或预测过程中，模型会首先对输入文本进行分词，再通过词表将每个 token 映射为其对应的 ID。接着，这些 ID 会被输入嵌入层，转换为**低维稠密的向量表示**，也就是词向量

![文本分词和构建词表|600x0](https://vip.123pan.cn/1844935313/obsidian/20251015203201267.png)

在文本生成任务中，模型的输出层会针对词表中的每个 token 生成一个概率分布，表示其作为下一个词的可能性。系统通常选取具有最大概率的ID，并通过词表查找对应的 token，从而逐步生成最终的输出文本。下图中输入文本为“我想”，该文本的下一个词概率明显较大的有“你”、“吃”、“去”，即“我想你”、“我想吃”、“我想去”。

![文本生成任务，预测下一个词|600x0](https://vip.123pan.cn/1844935313/obsidian/20251015203535035.png)

## 英文分词

按照分词粒度的大小，可分为词级（Word-Level）分词、字符级（Character‑Level）分词和子词级（Subword‑Level）分词。

英语的词级分词按照空格和标点分隔词语，这种分词虽然便于理解和实现，但在实际应用中容易出现 OOV（Out‑Of‑Vocabulary）问题。所谓 OOV，是指在模型使用阶段，输入文本中出现了不在预先构建词表中的词语，常见的有网络热词、专有名词、复合词及拼写变体等。由于模型无法识别这些词，通常会将其统一替换为特殊标记，如 `<UNK>`（Unknown Token），从而导致语义信息的丢失，影响模型的理解与预测能力。

字符级分词是以单个字符为最小单位进行分词的方法，文本中的每一个字母、数字、标点甚至空格，都会被视作一个独立的 token。在这种分词方式下，词表仅由所有可能出现的字符组成，因此词表规模非常小，覆盖率极高，几乎不存在 OOV问题。然而，由于单个字符本身语义信息极弱，模型必须依赖更长的上下文来推断词义和结构，这显著增加了建模难度和训练成本。此外，输入序列也会变得更长，影响模型效率。

子词级分词是一种介于词级分词与字符级分词之间的分词方法，它将词语切分为更小的单元——子词（subword），例如词根、前缀、后缀或常见词片段。与词级分词相比，子词分词可以显著缓解OOV问题；与字符级分词相比，它能更好地保留一定的语义结构。


