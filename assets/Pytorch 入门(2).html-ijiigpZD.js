import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as e,b as o,d as r,e as n,o as p}from"./app-DlezE97U.js";const c={};function l(s,t){return p(),i("div",null,[t[0]||(t[0]=e("h1",{id:"前言",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#前言"},[e("span",null,"前言")])],-1)),t[1]||(t[1]=e("p",null,[n("学习尚硅谷 b站的 "),e("a",{href:"https://www.bilibili.com/video/BV1k44LzPEhU",target:"_blank",rel:"noopener noreferrer"},"NLP 教程"),n("。")],-1)),o(" more "),t[2]||(t[2]=r('<h1 id="技术演进历史" tabindex="-1"><a class="header-anchor" href="#技术演进历史"><span>技术演进历史</span></a></h1><p>90 年代，随着计算能力的提升和语料资源的积累，统计方法逐渐成为主流。通过对大量文本数据进行概率建模，系统能够“学习”语言中的模式和规律。典型方法包括 n-gram 模型、隐马尔可夫模型(HMM)和最大熵模型。</p><p>进入 21 世纪，NLP 技术逐步引入传统机器学习方法，如逻辑回归、支持向量机 (SVM)、决策树、条件随机场(CRF)等。这些方法在命名实体识别、文本分类等任务上表现出色。在此阶段，特征工程成为关键环节，研究者需要设计大量手工特征来提升模型性能。</p><p>自2010年代中期开始，深度学习在NLP中迅速崛起。基于神经网络的模型RNN、LSTM、GRU等，取代了传统手工特征工程，能够从海量数据中自动提取语义表示。随后，Transformer架构的提出极大提升了语言理解与生成的能力，深度学习不仅在精度上实现突破，也推动了预训练语言模型（如GPT、BERT等）和迁移学习的发展，使NLP技术更通用、更强大。</p><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251014224728382.png" alt="RNN" tabindex="0" loading="lazy"><figcaption>RNN</figcaption></figure><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251014224924676.png" alt="LSTM（Long Short-Term Memory）" tabindex="0" loading="lazy"><figcaption>LSTM（Long Short-Term Memory）</figcaption></figure><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251014224958375.png" alt="GRU（Gated Recurrent Unit）" tabindex="0" loading="lazy"><figcaption>GRU（Gated Recurrent Unit）</figcaption></figure><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251014225051167.png" alt="Transformer" width="438" tabindex="0" loading="lazy"><figcaption>Transformer</figcaption></figure><h1 id="安装所需依赖" tabindex="-1"><a class="header-anchor" href="#安装所需依赖"><span>安装所需依赖</span></a></h1><p>该课程使用的 Python 版本是 3.12。安装所需依赖有：</p><ul><li><code>pytorch</code>：深度学习框架，主要用于模型的构建、训练与推理。</li><li><code>jieba</code>：高效的中文分词工具，用于对原始中文文本进行分词预处理。</li><li><code>gensim</code>：用于训练词向量模型（如 Word2Vec、FastText），提升模型对词语语义关系的理解。</li><li><code>transformers</code>：由 Hugging Face 提供的预训练模型库，用于加载和微调 BERT 等主流模型。</li><li><code>datasets</code>：Hugging Face 提供的数据处理库，用于高效加载和预处理大规模数据集。</li><li><code>TensorBoard</code>：可视化工具，用于展示训练过程中的损失函数、准确率等指标变化。</li><li><code>tqdm</code>：用于显示进度条，帮助实时监控训练与数据处理的进度。</li><li><code>Jupyter Notebook</code>：交互式开发环境，用于编写、测试和可视化模型代码与实验过程。</li></ul><h1 id="文本表示方法" tabindex="-1"><a class="header-anchor" href="#文本表示方法"><span>文本表示方法</span></a></h1><p>文本表示是将自然语言转化为计算机能够理解的数值形式。</p><p>早期的文本表示方法（如词袋模型）通常将整段文本编码为一个向量。这类方法实现简单、计算高效，但存在明显的局限性——表达语序和上下文语义的能力较弱。</p><p>因此，现代 NLP 技术逐渐引入更加精细和表达力更强的文本表示方法，以更有效地建模语言的结构和含义：</p><ul><li>分词（Tokenization）是将原始文本切分为若干具有独立语义的最小单元，即 <code>token</code> 的过程，是所有 NLP 任务的起点</li><li>词表（Vocabulary）是由语料库构建出的、包含模型可识别 token 的集合。词表中每个 token 都分配有唯一的 <code>ID</code>，并支持 token 与 ID 之间的双向映射</li><li>词向量：在训练或预测过程中，模型会首先对输入文本进行分词，再通过词表将每个 token 映射为其对应的 ID。接着，这些 ID 会被输入嵌入层，转换为<strong>低维稠密的向量表示</strong>，也就是词向量</li></ul><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251015203201267.png" alt="文本分词和构建词表" width="600" tabindex="0" loading="lazy"><figcaption>文本分词和构建词表</figcaption></figure><p>在文本生成任务中，模型的输出层会针对词表中的每个 token 生成一个概率分布，表示其作为下一个词的可能性。系统通常选取具有最大概率的ID，并通过词表查找对应的 token，从而逐步生成最终的输出文本。下图中输入文本为“我想”，该文本的下一个词概率明显较大的有“你”、“吃”、“去”，即“我想你”、“我想吃”、“我想去”。</p><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251015203535035.png" alt="文本生成任务，预测下一个词" width="600" tabindex="0" loading="lazy"><figcaption>文本生成任务，预测下一个词</figcaption></figure><h2 id="英文分词" tabindex="-1"><a class="header-anchor" href="#英文分词"><span>英文分词</span></a></h2><p>按照分词粒度的大小，可分为词级（Word-Level）分词、字符级（Character‑Level）分词和子词级（Subword‑Level）分词。</p><p>英语的词级分词按照空格和标点分隔词语，这种分词虽然便于理解和实现，但在实际应用中容易出现 OOV（Out‑Of‑Vocabulary）问题。所谓 OOV，是指在模型使用阶段，输入文本中出现了不在预先构建词表中的词语，常见的有网络热词、专有名词、复合词及拼写变体等。由于模型无法识别这些词，通常会将其统一替换为特殊标记，如 <code>&lt;UNK&gt;</code>（Unknown Token），从而导致语义信息的丢失，影响模型的理解与预测能力。</p><p>字符级分词是以单个字符为最小单位进行分词的方法，文本中的每一个字母、数字、标点甚至空格，都会被视作一个独立的 token。在这种分词方式下，词表仅由所有可能出现的字符组成，因此词表规模非常小，覆盖率极高，几乎不存在 OOV问题。然而，由于单个字符本身语义信息极弱，模型必须依赖更长的上下文来推断词义和结构，这显著增加了建模难度和训练成本。此外，输入序列也会变得更长，影响模型效率。</p><p>子词级分词是一种介于词级分词与字符级分词之间的分词方法，它将词语切分为更小的单元——子词（subword），例如词根、前缀、后缀或常见词片段。与词级分词相比，子词分词可以显著缓解OOV问题；与字符级分词相比，它能更好地保留一定的语义结构。</p>',24))])}const h=a(c,[["render",l]]),m=JSON.parse('{"path":"/programming%20languagae/Pytorch%20%E5%85%A5%E9%97%A8(2).html","title":"Pytorch 入门(2)","lang":"zh-CN","frontmatter":{"title":"Pytorch 入门(2)","date":"2025-10-14T00:00:00.000Z","tags":["Pytorch","尚硅谷"],"category":["Python"],"description":"学习尚硅谷 b站的 NLP 教程。","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Pytorch 入门(2)\\",\\"image\\":[\\"https://vip.123pan.cn/1844935313/obsidian/20251014224728382.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251014224924676.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251014224958375.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251014225051167.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251015203201267.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251015203535035.png\\"],\\"datePublished\\":\\"2025-10-14T00:00:00.000Z\\",\\"dateModified\\":\\"2025-10-17T08:15:07.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"庸碌无常\\",\\"url\\":\\"https://pluinyiasnhg.top\\"}]}"],["meta",{"property":"og:url","content":"https://pluinyiasnhg.top/programming%20languagae/Pytorch%20%E5%85%A5%E9%97%A8(2).html"}],["meta",{"property":"og:site_name","content":"庸碌无常的博客"}],["meta",{"property":"og:title","content":"Pytorch 入门(2)"}],["meta",{"property":"og:description","content":"学习尚硅谷 b站的 NLP 教程。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://vip.123pan.cn/1844935313/obsidian/20251014224728382.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-10-17T08:15:07.000Z"}],["meta",{"property":"article:tag","content":"尚硅谷"}],["meta",{"property":"article:tag","content":"Pytorch"}],["meta",{"property":"article:published_time","content":"2025-10-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-10-17T08:15:07.000Z"}]]},"git":{"createdTime":1760453574000,"updatedTime":1760688907000,"contributors":[{"name":"pluinyiasnhg","username":"pluinyiasnhg","email":"pluinyiasnhg@gmail.com","commits":2,"url":"https://github.com/pluinyiasnhg"}]},"readingTime":{"minutes":5.01,"words":1502},"filePathRelative":"programming languagae/Pytorch 入门(2).md","excerpt":"\\n<p>学习尚硅谷 b站的 <a href=\\"https://www.bilibili.com/video/BV1k44LzPEhU\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">NLP 教程</a>。</p>\\n","autoDesc":true}');export{h as comp,m as data};
