import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as l,a as s,b as t,d as e,e as a,o as h}from"./app-Ci_uHeur.js";const p={};function k(d,i){return h(),l("div",null,[i[0]||(i[0]=s("h1",{id:"前言",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#前言"},[s("span",null,"前言")])],-1)),i[1]||(i[1]=s("p",null,[a("学习尚硅谷的 "),s("a",{href:"https://www.bilibili.com/video/BV1k44LzPEhU",target:"_blank",rel:"noopener noreferrer"},"NLP 教程"),a("。")],-1)),t(" more "),i[2]||(i[2]=e(`<h1 id="技术演进历史" tabindex="-1"><a class="header-anchor" href="#技术演进历史"><span>技术演进历史</span></a></h1><p>90 年代，随着计算能力的提升和语料资源的积累，统计方法逐渐成为主流。通过对大量文本数据进行概率建模，系统能够“学习”语言中的模式和规律。典型方法包括 n-gram 模型、隐马尔可夫模型(HMM)和最大熵模型。</p><p>进入 21 世纪，NLP 技术逐步引入传统机器学习方法，如逻辑回归、支持向量机 (SVM)、决策树、条件随机场(CRF)等。这些方法在命名实体识别、文本分类等任务上表现出色。在此阶段，特征工程成为关键环节，研究者需要设计大量手工特征来提升模型性能。</p><p>自2010年开始，深度学习在NLP中迅速崛起。基于神经网络的模型，比如 RNN、LSTM、GRU等，取代了传统手工特征工程，能够从海量数据中自动提取语义表示。随后，Transformer 架构的提出极大提升了语言理解与生成的能力，深度学习不仅在精度上实现突破，也推动了预训练语言模型（如GPT、BERT等）和迁移学习的发展，使 NLP 技术更通用、更强大。</p><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251014224728382.png" alt="RNN" tabindex="0" loading="lazy"><figcaption>RNN</figcaption></figure><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251014224924676.png" alt="LSTM（Long Short-Term Memory）" tabindex="0" loading="lazy"><figcaption>LSTM（Long Short-Term Memory）</figcaption></figure><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251014224958375.png" alt="GRU（Gated Recurrent Unit）" tabindex="0" loading="lazy"><figcaption>GRU（Gated Recurrent Unit）</figcaption></figure><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251014225051167.png" alt="Transformer" width="438" tabindex="0" loading="lazy"><figcaption>Transformer</figcaption></figure><h1 id="安装所需依赖" tabindex="-1"><a class="header-anchor" href="#安装所需依赖"><span>安装所需依赖</span></a></h1><p>该课程使用的 Python 版本是 3.12。安装所需依赖有：</p><ul><li><code>pytorch</code>：深度学习框架，主要用于模型的构建、训练与推理。</li><li><code>jieba</code>：高效的中文分词工具，用于对原始中文文本进行分词预处理。</li><li><code>gensim</code>：用于训练词向量模型（如 Word2Vec、FastText），提升模型对词语语义关系的理解。可以使用<a href="https://github.com/Embedding/Chinese-Word-Vectors" target="_blank" rel="noopener noreferrer">公开的中文词向量</a></li><li><code>transformers</code>：由 Hugging Face 提供的预训练模型库，用于加载和微调 BERT 等主流模型。</li><li><code>datasets</code>：Hugging Face 提供的数据处理库，用于高效加载和预处理大规模数据集。</li><li><code>tensorboard</code>：可视化工具，用于展示训练过程中的损失函数、准确率等指标变化。</li><li><code>tqdm</code>：用于显示进度条，帮助实时监控训练与数据处理的进度。</li><li><code>Jupyter Notebook</code>：交互式开发环境，用于编写、测试和可视化模型代码与实验过程。</li><li><code>scikit-learn</code> ：机器学习工具。可以用来划分数据集。</li></ul><h1 id="文本表示方法" tabindex="-1"><a class="header-anchor" href="#文本表示方法"><span>文本表示方法</span></a></h1><p>文本表示是将自然语言转化为计算机能够理解的数值形式。</p><p>早期的文本表示方法（如词袋模型）通常将整段文本编码为一个向量。这类方法实现简单、计算高效，但存在明显的局限性——表达语序和上下文语义的能力较弱。</p><p>因此，现代 NLP 技术逐渐引入更加精细和表达力更强的文本表示方法，以更有效地建模语言的结构和含义：</p><ul><li>分词（Tokenization）是将原始文本切分为若干具有独立语义的最小单元，即 <code>token</code> 的过程，是所有 NLP 任务的起点</li><li>词表（Vocabulary）是由语料库构建出的、包含模型可识别 token 的集合。词表中每个 token 都分配有唯一的 <code>ID</code>，并支持 token 与 ID 之间的双向映射</li><li>词向量：在训练或预测过程中，模型会首先对输入文本进行分词，再通过词表将每个 token 映射为其对应的 ID。接着，这些 ID 会被输入嵌入层，转换为<strong>低维稠密的向量表示</strong>，也就是词向量。ID 可以用 one-hot 向量表示，通过与词表进行矩阵乘法，抽出对应的词向量</li></ul><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251015203201267.png" alt="文本分词和构建词表" width="600" tabindex="0" loading="lazy"><figcaption>文本分词和构建词表</figcaption></figure><p>在文本生成任务中，模型的输出层会针对词表中的每个 token 生成一个概率分布，表示其作为下一个词的可能性。系统通常选取具有最大概率的ID，并通过词表查找对应的 token，从而逐步生成最终的输出文本。下图中输入文本为“我想”，该文本的下一个词概率明显较大的有“你”、“吃”、“去”，即“我想你”、“我想吃”、“我想去”。</p><p>这一过程很像是在查字典，拿到一个句子，里面有一些不认识的单词，识别出陌生单词的行为就是分词。通过单词查询字典对应条目所在的页数，页数即为 ID。通过页码查看该单词详细的释义，释义即是词向量。字典的解释总是比单词要多上许多内容，模型通过这些内容进而更好的理解陌生单词，从而把握整句话的含义。</p><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251015203535035.png" alt="文本生成任务，预测下一个词" width="600" tabindex="0" loading="lazy"><figcaption>文本生成任务，预测下一个词</figcaption></figure><p>词向量有静态与动态之分。静态词向量只为每个词分配一个<strong>固定的向量表示</strong>，不论它在句中出现的语境如何。然而，语言的表达极其灵活，一个词在不同上下文中可能有完全不同的含义。这就推动了上下文相关的词表示的发展。上下文相关词表示（Contextual Word Representations），是指词语的向量表示会根据它所在的句子上下文动态变化，从而更好地捕捉其语义。一个具有代表性的模型是——<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener noreferrer">ELMo</a>。</p><h2 id="英文分词" tabindex="-1"><a class="header-anchor" href="#英文分词"><span>英文分词</span></a></h2><p>按照分词粒度的大小，可分为词级（Word-Level）分词、字符级（Character‑Level）分词和子词级（Subword‑Level）分词。</p><p>英语的词级分词按照空格和标点分隔词语，这种分词虽然便于理解和实现，但在实际应用中容易出现 OOV（Out‑Of‑Vocabulary）问题。所谓 OOV，是指在模型使用阶段，输入文本中出现了不在预先构建词表中的词语，常见的有网络热词、专有名词、复合词及拼写变体等。由于模型无法识别这些词，通常会将其统一替换为特殊标记，如 <code>&lt;UNK&gt;</code>（Unknown Token），从而导致语义信息的丢失，影响模型的理解与预测能力。</p><p>字符级分词是以单个字符为最小单位进行分词的方法，文本中的每一个字母、数字、标点甚至空格，都会被视作一个独立的 token。在这种分词方式下，词表仅由所有可能出现的字符组成，因此词表规模非常小，覆盖率极高，几乎不存在 OOV问题。然而，由于单个字符本身语义信息极弱，模型必须依赖更长的上下文来推断词义和结构，这显著增加了建模难度和训练成本。此外，输入序列也会变得更长，影响模型效率。</p><p>子词级分词是一种介于词级分词与字符级分词之间的分词方法，它将词语切分为更小的单元——子词（subword），例如词根、前缀、后缀或常见词片段。与词级分词相比，子词分词可以显著缓解OOV问题；与字符级分词相比，它能更好地保留一定的语义结构。</p><h2 id="中文分词" tabindex="-1"><a class="header-anchor" href="#中文分词"><span>中文分词</span></a></h2><p>类似英文分词，中文分词也能按照三个等级进行划分：</p><ul><li>字符级分词：一个汉字作为一个 token</li><li>词级分词：一个词语作为一个 token。由于中文词语不像英文那样可以用空格进行区分，所以需要依赖词典、规则或模型来识别词语边界</li><li>子词级分词：以汉字为基本单位，通过学习语料中高频的字组合（如“自然”、“语言”、“处理”），自动构建子词词表</li></ul><p>中文分词工具，按照实现方式分为如下两类：</p><ul><li>一类是基于<strong>词典或模型</strong>的传统方法，主要以“词”为单位进行切分。比如 <a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener noreferrer">jieba</a>、<a href="https://github.com/hankcs/HanLP" target="_blank" rel="noopener noreferrer">HanLP</a></li><li>另一类是基于<strong>子词建模算法</strong>（如BPE）的方式，从数据中自动学习高频字组合，构建子词词表。比如 <a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener noreferrer">Hugging Face Tokenizer</a>、<a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener noreferrer">SentencePiece</a>、<a href="https://github.com/openai/tiktoken" target="_blank" rel="noopener noreferrer">tiktoken</a></li></ul><h1 id="rnn" tabindex="-1"><a class="header-anchor" href="#rnn"><span>RNN</span></a></h1><p>RNN（循环神经网络）的核心结构是一个具有循环连接的隐藏层，它以时间步（time step）为单位，依次处理输入序列中的每个 token。</p><p>在每个时间步，RNN 接收当前 token 的向量和上一个时间步的隐藏状态（即隐藏层的输出），计算并生成新的隐藏状态，并将其传递到下一时间步。其公式为：</p><p>$$h_t=tanh(x_tW_x+h_{t-1}W_h+b)$$</p><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251101095651999.png" alt="RNN 的 seqlen、input_size、hidden_size" width="650" tabindex="0" loading="lazy"><figcaption>RNN 的 seqlen、input_size、hidden_size</figcaption></figure><ul><li><code>seq_len</code> 为4，即从 $x_1$ 到 $x_4$ ，是序列长度</li><li><code>input_size</code> 为3，即每个 $x_t$ 的长度，是每个时间步输入特征的维度（词向量维度）</li><li><code>hidden_size</code> 为4，即图中右侧的四个圆。</li></ul><h2 id="单层结构" tabindex="-1"><a class="header-anchor" href="#单层结构"><span>单层结构</span></a></h2><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251101100158825.png" alt="单层结构 RNN" width="650" tabindex="0" loading="lazy"><figcaption>单层结构 RNN</figcaption></figure><p>注意，图中单层结构 RNN 展开后有4个包含激活函数 tanh 的层，但实际上他们是共用一个层。</p><h2 id="多层结构" tabindex="-1"><a class="header-anchor" href="#多层结构"><span>多层结构</span></a></h2><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251101100354469.png" alt="多层结构 RNN" width="650" tabindex="0" loading="lazy"><figcaption>多层结构 RNN</figcaption></figure><p>图中形如 $h_0^1$ 的符号，其下标表示时间步，上标表示 RNN 的层数。如果把图中右侧的展开图视为一个原点在左下角的平面坐标系，那么坐标系的 x 轴是时间步，y 轴是层数。</p><h2 id="双向结构" tabindex="-1"><a class="header-anchor" href="#双向结构"><span>双向结构</span></a></h2><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251101100809508.png" alt="双向结构 RNN" width="650" tabindex="0" loading="lazy"><figcaption>双向结构 RNN</figcaption></figure><p>前面提到的 RNN 都是单向结构的，在每个时间步只输出一个隐藏状态，该状态仅包含来自上文的信息。为了充分利用当前词之后的下文，即利用上下文内容，人们设计了双向结构的 RNN，反映在上图中就是，有两个方向的箭头：一个从左侧进入激活函数 tanh，一个从右侧进入激活函数 tanh。不过两个 tanh 所在的层不是同一个，出于训练效率的考虑，正向和逆向的两个层应该并行训练。</p><p>最后，正向 RNN 和 逆向 RNN 通过简单的拼接（concat）作为下一层的输入，如果 RNN 是多层结构。</p><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251101143912603.png" alt="input、output、h_n 的形状" tabindex="0" loading="lazy"><figcaption>input、output、h_n 的形状</figcaption></figure><h1 id="rnn-实践" tabindex="-1"><a class="header-anchor" href="#rnn-实践"><span>RNN 实践</span></a></h1><h2 id="数据集处理" tabindex="-1"><a class="header-anchor" href="#数据集处理"><span>数据集处理</span></a></h2><p>数据集下载地址：<a href="https://huggingface.co/datasets/Jax-dan/HundredCV-Chat" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/Jax-dan/HundredCV-Chat</a>。也可以从 huggingface 的镜像地址下载：<a href="https://hf-mirror.com/datasets/Jax-dan/HundredCV-Chat" target="_blank" rel="noopener noreferrer">https://hf-mirror.com/datasets/Jax-dan/HundredCV-Chat</a>。</p><p>数据集文件的格式为 <code>.jsonl</code>，<code>jsonl</code> 文件的每一行可以看作一个 <code>json</code> 文件，下面代码来自数据集文件的第一行。本任务中只需用到数据集中的对话文本。</p><div class="language-json line-numbers-mode" data-highlighter="shiki" data-ext="json" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-json"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    &quot;topic&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;校园生活分享&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    &quot;user1&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;李欣怡&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    &quot;user2&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;杨欢&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">    &quot;dialog&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: [</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：杨欢，最近校园里有什么新鲜事吗？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：嗨，李欣怡！我们学校刚刚举办了一次科技节，很多学生展示了他们的发明。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：听起来好有趣！我这边的学校正在筹备一场文化节，主要是推广传统文化。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：文化节听起来也很棒。你们会做哪些活动呢？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：我们打算办个书法展和传统服饰秀，还会请来一些老师教大家制作传统小吃。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：真不错！我们科技节上有同学展示了一款智能垃圾分类箱，挺受欢迎的。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：这创意真好。我们也应该多关注环保问题，比如组织一次清洁校园的活动。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：对，我也觉得这样做很有意义。你们学校有类似的社团或小组吗？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：有的，我们有一个志愿者服务队，经常会组织这样的活动。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：那真是太好了！你平时是怎么平衡学业和这些课外活动的呢？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：我会合理安排时间，把学习放在第一位，然后是重要的活动。你觉得呢？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：我也是这样做的。我觉得找到自己的兴趣点很重要，这样才能更有动力去做好每一件事。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：完全同意！对了，你参加过模拟联合国社团吗？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：参加过，挺有意思的。你呢？有没有类似的经历？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：有啊！我还当过中国代表团的代表，提出了很多关于可持续发展的建议。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：真厉害！你在这些活动中一定学到了不少东西吧？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：当然了，不仅增长见识还锻炼了我的团队合作能力。你也一样吧？&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：是的，我也从中受益匪浅，特别是编程能力和解决问题的能力得到了提升。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user1：希望以后我们能有更多机会一起交流学习经验。&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;user2：绝对同意！我们可以互相分享更多的校园活动信息和心得。&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    ]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>为了构造适用于“下一词预测”任务的训练样本，首先需要对原始语料进行分词。随后，采用<strong>滑动窗口</strong>的方式，从分词后的序列中提取连续的上下文片段，并以每个窗口的下一个词作为预测目标，构成输入-输出对，如下图所示：</p><figure><img src="https://vip.123pan.cn/1844935313/obsidian/20251101155736341.png" alt="用上文的5个词预测下一个词" width="400" tabindex="0" loading="lazy"><figcaption>用上文的5个词预测下一个词</figcaption></figure><h2 id="模型结构设计" tabindex="-1"><a class="header-anchor" href="#模型结构设计"><span>模型结构设计</span></a></h2><p>本任务采用基于 RNN 的语言模型结构来实现“下一词预测”功能。模型整体由以下三个主要部分组成：</p><ol><li>嵌入层。将输入的词或字索引映射为稠密向量表示，便于后续神经网络处理。</li><li>RNN。用于建模输入序列的上下文信息，输出最后一个时间步的隐藏状态作为上下文表示。</li><li>输出层。这里是线性层，将隐藏状态映射到词表大小的维度，生成对下一个词的概率预测。</li></ol><h2 id="训练方案" tabindex="-1"><a class="header-anchor" href="#训练方案"><span>训练方案</span></a></h2><ul><li>损失函数选择 CrossEntropyLoss。预测下一词功能，可以看作多分类问题</li><li>优化器选择 Adam</li></ul><h2 id="项目结构" tabindex="-1"><a class="header-anchor" href="#项目结构"><span>项目结构</span></a></h2><div class="language-zsh line-numbers-mode" data-highlighter="shiki" data-ext="zsh" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-zsh"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">input_method</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> data</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">              # 数据目录</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">│  </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> ├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> processed</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">     # 预处理后的数据</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">│  </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> └──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> raw</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">           # 原始数据</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> logs</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">              # 训练日志</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> models</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">            # 保存训练好的模型参数</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">└──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> src</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">               # 源码目录</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> config.py</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">     # 超参数配置</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> dataset.py</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 自定义Dataset</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> evaluate.py</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">   # 模型评估脚本，存放评价指标</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> model.py</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">      # 模型结构定义</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> predict.py</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 模型推理脚本</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> process.py</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 数据预处理脚本</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ├──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> tokenizer.py</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # 自定义分词器</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    └──</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> train.py</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">      # 模型训练脚本</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> os</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pandas </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">as</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pd</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">CURRENT_DIR</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> =</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> os.path.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">dirname</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(os.path.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">abspath</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">__file__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> process</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">():</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;开始处理数据&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 1.读取文件</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    jsonl_path </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> os.path.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">join</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">CURRENT_DIR</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;../data/raw/synthesized_.jsonl&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    df </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> pd.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">read_json</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(jsonl_path, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">lines</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">orient</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;records&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(df.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">head</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">())</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;数据处理完成&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;"> __name__</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> ==</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;__main__&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">    process</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>词表基于训练集构建，<br> 需要额外添加一个 <code>&lt;UNK&gt;</code> ，<br> 词表必须有序。<br> 词表保存在 <code>models</code> 目录下，和模型参数放一个地方。<br> 词表中每一行是一个 ID，ID 之间用换行符分隔，可以视为一个大字符串，字符之间用 <code>\\n</code> 分割。<br> 构建词表时，最费时的阶段是给句子分词，可以用 tqdm 显示进度条，以及用 <code>df_sample = df.sample(frac=0.1)</code> 抽样，抽取十分之一的数据。</p><p><a href="http://model.py" target="_blank" rel="noopener noreferrer">model.py</a><br><code>last_hidden_state = output[:, -1, :]</code> 省略掉第二个维度，因为该维度的大小为1</p><p><a href="http://train.py" target="_blank" rel="noopener noreferrer">train.py</a></p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">with</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> open</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(config.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">MODEL_PATH</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> /</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;vocab.txt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;r&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">encoding</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;utf-8&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">as</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> f:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">	vocab_list </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> f.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">readlines</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(vocab_list[:</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">5</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">])</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-zsh line-numbers-mode" data-highlighter="shiki" data-ext="zsh" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-zsh"><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;&lt;unk&gt;\\n&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;交通管理\\n&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;福音\\n&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;RFM\\n&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;钉\\n&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>tensorboard 可以实时检测训练过程。<br> 通过设置层级目录，tensorboard 可以在一张图上显示多次实验的结果，用来进行比较</p><p><a href="http://predict.py" target="_blank" rel="noopener noreferrer">predict.py</a></p><p><code>input_tensor = torch.tensor([indexs], dtype=torch.long).to(device)</code> 这里的 <code>[]</code> 用于增加一个维度，使张量形状从 <code>[seq_len]</code> 变为 <code>[1, seqlen]</code><br> 预测脚本的模拟应用：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> predict</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">text</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 1. 设备</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    device </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;cuda&quot;</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> if</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.cuda.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">is_available</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">() </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">else</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;cpu&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 2. 词表</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    with</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> open</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(config.</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">MODEL_PATH</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> /</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;vocab.txt&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;r&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">encoding</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;utf-8&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">as</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> f:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        vocab_list </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [line.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">strip</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\\n</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> line </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> f.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">readlines</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    word2index </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> {word: index </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> index, word </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> enumerate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(vocab_list)}</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    index2word </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> {index: word </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> index, word </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> enumerate</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(vocab_list)}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 3. 模型</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    model </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> InputMethodModel</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">vocab_size</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">len</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(vocab_list)).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(device)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 4. 处理输入</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    tokens </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> jieba.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">lcut</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(text)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    indexs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [word2index.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">get</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(token, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> token </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> tokens]</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # input_tensor = torch.tensor(indexs, dtype=torch.long).unsqueeze(0)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    input_tensor </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">tensor</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">([indexs], </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">dtype</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">torch.long).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">to</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(device)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # 5. 预测逻辑</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">eval</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    with</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">no_grad</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">():</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        output </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> model</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(input_tensor)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        # output.shape: [batch_size, vocab_size]</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    top5_indexes </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">topk</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(output, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">k</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">5</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).indices</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    # top5_indexes.shape: [batch_size, 5]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    top5_indexes_list </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> top5_indexes.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">tolist</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    top5_tokens </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  [index2word.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">get</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(index) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> index </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> top5_indexes_list[</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]]</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> top5_tokens</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;"> __name__</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;"> ==</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;__main__&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    top5_tokens </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> predict</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;我们团队&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    print</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(top5_tokens)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><a href="http://evaluate.py" target="_blank" rel="noopener noreferrer">evaluate.py</a></p><p><code>evaluate</code> 函数有调用 <code>predict.py</code> 中 <code>predict_batch</code> 函数，后者里面已经有 <code>model.eval()</code> 和 <code>with torch.no_grad():</code> ，所以 <code>evaluate</code> 函数开头两行不用写这两句。</p><p><code>target = target.tolist()</code> ：该评估脚本不需要计算损失，因此 <code>target</code> 也不用放到 gpu 上。</p><p>pytorch 中 <code>topk</code> 默认按从大到小排序</p><p><a href="http://tokenizer.py" target="_blank" rel="noopener noreferrer">tokenizer.py</a></p><p>将词表操作相关的代码抽取出来，放到 <code>tokenizer.py</code> 中。</p><p>对象 JiebaTokenizer：</p><p>属性：</p><ul><li><code>vocab_list</code> : <code>list[str]</code> 词表列表</li><li><code>vocab_size</code> : <code>int</code> 词表大小</li><li><code>word2index</code> : <code>dict[str, int]</code> word 到 index 的映射</li><li><code>index2word</code> : <code>dict[int, str]</code> index 到 word 的映射</li><li><code>unk_token</code> : <code>str</code> 未登录词，比如 <code>&lt;unk&gt; &lt;begin&gt; &lt;end&gt;</code> ，这是个类属性</li><li><code>unk_token_index</code> : <code>int</code> 未登录词 index</li></ul><p>方法：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">@</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">staticmethod</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> tokenize</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">sentence</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#986801;--shiki-dark:#ABB2BF;"> Any</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) -&gt; list[</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">分词 </span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">将 jieba.lcut() 封装到列表中</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:parma sentence: 句子</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:return: token列表</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">@</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">classmethod</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> build_vocab</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">cls</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">				sentence: list[</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">],</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">				vocab_file: Any)</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">构建并保存词表</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:parma sentence: 句子</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:parma vocab_file: 词表文件路径</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">@</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">classmethod</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> from_vocab</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">cls</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, vocab_file: Any) </span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">-&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> JiebaTokenizer</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">加载词表并构建Tokenizer对象</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">构造方法，封装了 __init_()，使用起来更方便</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:parma vocab_file: 词表文件</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:return: tokenizer对象</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> __init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, vocab_list: list[</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]) </span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">-&gt;</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> None</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">初始化tokenizer</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:parma vocab_list: 词表列表</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> encode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, sentence: Any) </span><span style="--shiki-light:white;--shiki-dark:#FFFFFF;">-&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> list[</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">int</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">编码</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">相当于把分词方法 tokenize 和属性 word2index 合二为一</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:parma sentence: 句子</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">:return: index列表</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;&quot;&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>build_vocab</code> 是一个类方法，因为创建 <code>JiebaTokenizer</code> 类时需要传入词表，创建词表的方法 <code>build_vocab</code> 需要先实例化 <code>JiebaTokenizer</code> 。如果把 <code>build_vocab</code> 设置为静态方法，那么它就无法使用 <code>self</code> 获取 <code>unk_token</code> 。设置为类方法后，<code>build_token</code> 也没法用 self 获取 unk_token，这时把 unk_token 设置为类属性。</p><p>实例方法 类方法 静态方法</p>`,85))])}const c=n(p,[["render",k]]),g=JSON.parse('{"path":"/programming%20languagae/Pytorch%20%E5%85%A5%E9%97%A8(2).html","title":"Pytorch 入门(2)","lang":"zh-CN","frontmatter":{"title":"Pytorch 入门(2)","date":"2025-10-14T00:00:00.000Z","tags":["Pytorch","尚硅谷"],"category":["Python"],"description":"学习尚硅谷的 NLP 教程。","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Pytorch 入门(2)\\",\\"image\\":[\\"https://vip.123pan.cn/1844935313/obsidian/20251014224728382.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251014224924676.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251014224958375.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251014225051167.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251015203201267.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251015203535035.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251101095651999.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251101100158825.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251101100354469.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251101100809508.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251101143912603.png\\",\\"https://vip.123pan.cn/1844935313/obsidian/20251101155736341.png\\"],\\"datePublished\\":\\"2025-10-14T00:00:00.000Z\\",\\"dateModified\\":\\"2025-11-16T07:10:09.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"庸碌无常\\",\\"url\\":\\"https://pluinyiasnhg.top\\"}]}"],["meta",{"property":"og:url","content":"https://pluinyiasnhg.top/programming%20languagae/Pytorch%20%E5%85%A5%E9%97%A8(2).html"}],["meta",{"property":"og:site_name","content":"庸碌无常的博客"}],["meta",{"property":"og:title","content":"Pytorch 入门(2)"}],["meta",{"property":"og:description","content":"学习尚硅谷的 NLP 教程。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://vip.123pan.cn/1844935313/obsidian/20251014224728382.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-11-16T07:10:09.000Z"}],["meta",{"property":"article:tag","content":"尚硅谷"}],["meta",{"property":"article:tag","content":"Pytorch"}],["meta",{"property":"article:published_time","content":"2025-10-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-11-16T07:10:09.000Z"}]]},"git":{"createdTime":1760453574000,"updatedTime":1763277009000,"contributors":[{"name":"pluinyiasnhg","username":"pluinyiasnhg","email":"pluinyiasnhg@gmail.com","commits":3,"url":"https://github.com/pluinyiasnhg"}]},"readingTime":{"minutes":14.93,"words":4480},"filePathRelative":"programming languagae/Pytorch 入门(2).md","excerpt":"\\n<p>学习尚硅谷的 <a href=\\"https://www.bilibili.com/video/BV1k44LzPEhU\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">NLP 教程</a>。</p>\\n","autoDesc":true}');export{c as comp,g as data};
